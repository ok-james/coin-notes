(window.webpackJsonp=window.webpackJsonp||[]).push([[27],{483:function(a,e,t){"use strict";t.r(e);var s=t(65),v=Object(s.a)({},(function(){var a=this,e=a.$createElement,t=a._self._c||e;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("p",[a._v("比特币挖矿算法本身没有什么漏洞，但是，在算法之外，有一个饱受争议的问题，就是挖矿设备的专业化，主要是指专门用于挖矿的 ASIC 芯片。很多人认为这个与去中心化的理念是背道而驰的，也是跟比特币的设计初衷是相违背的。")]),a._v(" "),t("p",[a._v("而以太坊希望实现 ASIC resistance ，也就是避免挖矿设备的专业化。")]),a._v(" "),t("p",[a._v("那如何能够设计出一个对 ASIC 芯片不友好的挖矿方式呢？一种常见的做法是增加挖矿对于内存的需求，也就是 memory hard mining puzzle 。ASIC 芯片相比于普通计算机来说，其优点是算力强，但是在内存访问的性能上没有优势。")]),a._v(" "),t("h2",{attrs:{id:"以太坊的-memory-puzzle"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#以太坊的-memory-puzzle"}},[a._v("#")]),a._v(" 以太坊的 memory puzzle")]),a._v(" "),t("p",[a._v("以太坊使用的是一大一小两个数据集，小的是一个 16M 的 cache ，大的是一个 1G 的 dataset ，称为 DAG ，这 1G 的数据集是从 16M 的 cache 中生成出来的。")]),a._v(" "),t("p",[a._v("为什么要设计成一大一小两个数据集？就是为了便于验证，轻节点只需要保存 16M 的 cache 即可，只有需要挖矿的全节点才需要保存 1G 的数据集。")]),a._v(" "),t("p",[a._v("另外，需要注意，这里提到的 16M 和 1G 这两个大小是会改变的，每产生固定数目的区块以后，就会提高这两个数值。")]),a._v(" "),t("h3",{attrs:{id:"基本思想"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#基本思想"}},[a._v("#")]),a._v(" 基本思想")]),a._v(" "),t("p",[a._v("如果是全节点，则下面的所有过程都要执行一遍，如果是轻节点，则只需要执行第一步【生成 16M 的 cache】即可。")]),a._v(" "),t("h4",{attrs:{id:"生成-16m-的-cache"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#生成-16m-的-cache"}},[a._v("#")]),a._v(" 生成 16M 的 cache")]),a._v(" "),t("p",[a._v("首先需要生成 16M 的 cache ，生成的过程如下：")]),a._v(" "),t("p",[a._v("首先从一个种子 seed 节点开始，经过运算，得到 16M cache 的第一个元素")]),a._v(" "),t("p",[a._v("然后从第一个元素开始，取哈希得到第二个元素，第二个元素取哈希得到第三个元素，以此类推，直到产生 16M cache 的元素为止。")]),a._v(" "),t("p",[a._v("每隔 30000 个块会重新生成 seed （对原来的 seed 求哈希值），并利用新的 seed 生成新的 cache 。")]),a._v(" "),t("p",[a._v("cache 的初始大小为 16M ，每隔 30000 哥块重新生成时增大初始大小的 1/128 —— 128K。")]),a._v(" "),t("h4",{attrs:{id:"生成-1g-的-dataset"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#生成-1g-的-dataset"}},[a._v("#")]),a._v(" 生成 1G 的 dataset")]),a._v(" "),t("p",[a._v("从 16M 的 cache 取出一个数，经过一些计算后，得到下一个数的位置，然后取出该位置的数，再经过相同的计算，得到再下一个数的位置，一共计算 256 次，最后得到的值，作为 1G 的 dataset 中第一个位置的值，然后重复上述的过程，直到将 1G 的 dataset 填满为止。")]),a._v(" "),t("p",[a._v("这个 dataset 叫做 DAG ，初始大小是 1G ，也是每隔 30000 个块更新，同时增大初始大小 1/128——8M。")]),a._v(" "),t("h4",{attrs:{id:"挖矿"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#挖矿"}},[a._v("#")]),a._v(" 挖矿")]),a._v(" "),t("p",[a._v("矿工只会用到 1G 的 dataset 。")]),a._v(" "),t("p",[a._v("具体流程是，初始情况下，根据 block header 和 nonce 计算出一个哈希，并作为 1G dataset 的位置读取出该位置以及下一个位置的值，然后对这两个值执行一些计算，从而得到下一个位置，这样循环计算 64 次，最终，用最后一次得到的哈希值与挖矿难度的目标阈值进行比较，确定是否符合挖矿难度。")])])}),[],!1,null,null,null);e.default=v.exports}}]);